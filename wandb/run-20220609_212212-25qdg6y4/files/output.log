GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 0
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
Traceback (most recent call last):
  File "train_cached.py", line 93, in <module>
    main()
  File "train_cached.py", line 89, in main
    trainer.fit(model, datamodule=dm, ckpt_path=config.get('checkpoint'))
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in fit
    self._call_and_handle_interrupt(
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 811, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1217, in _run
    self.strategy.setup(self)
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 159, in setup
    self._share_information_to_prevent_deadlock()
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 396, in _share_information_to_prevent_deadlock
    self._share_pids()
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 414, in _share_pids
    pids = self.all_gather(torch.tensor(os.getpid(), device=self.root_device))
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/strategies/parallel.py", line 111, in all_gather
    return all_gather_ddp_if_available(tensor, group=group, sync_grads=sync_grads)
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 188, in all_gather_ddp_if_available
    return AllGatherGrad.apply(tensor, group)
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 154, in forward
    gathered_tensor = [torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())]
  File "/root/code/OpenGlue/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py", line 154, in <listcomp>
    gathered_tensor = [torch.zeros_like(tensor) for _ in range(torch.distributed.get_world_size())]
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
