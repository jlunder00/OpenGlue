{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e95d3f-d877-4928-94a9-929c80650851",
   "metadata": {},
   "source": [
    "## Repository\n",
    "Clone https://github.com/jlunder00/OpenGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770da9f-0e24-4650-ab82-a3eff22e4085",
   "metadata": {},
   "source": [
    "## Data\n",
    "First, download/unzip data as shown in README.md:\n",
    "\n",
    "### Steps to prepare MegaDepth dataset for training\n",
    "1) Create folder `MegaDepth`, where your dataset will be stored.\n",
    "   ```\n",
    "   mkdir MegaDepth && cd MegaDepth\n",
    "   ```\n",
    "2) Download and unzip `MegaDepth_v1.tar.gz` from official [link](https://www.cs.cornell.edu/projects/megadepth/dataset/Megadepth_v1/MegaDepth_v1.tar.gz).\n",
    "You should now be able to see `MegaDepth/phoenix` directory.\n",
    "3) We provide the lists of pairs for training and validation, [link](https://drive.google.com/file/d/1DQl6N1bKEdzlRteCVMS1bWffz0SU-L9x/view?usp=sharing) to download. Each line corresponds to one pair and has the following structure:\n",
    "```\n",
    "path_image_A path_image_B exif_rotationA exif_rotationB [KA_0 ... KA_8] [KB_0 ... KB_8] [T_AB_0 ... T_AB_15] overlap_AB\n",
    "```\n",
    "`overlap_AB` - is a value of overlap between two images of the same scene, it shows how close (in position transformation) two images are. \n",
    "\n",
    "The resulting directory structure should be as follows:\n",
    "```\n",
    "MegaDepth/\n",
    "   - pairs/\n",
    "   |   - 0000/\n",
    "   |   |   - sparse-txt/\n",
    "   |   |   |    pairs.txt\n",
    "      ...\n",
    "   - phoenix/S6/zl548/MegaDepth_v1/\n",
    "   |   -0000/\n",
    "   |   |   - dense0/\n",
    "   |   |   |   - depths/\n",
    "   |   |   |   |   id.h5\n",
    "                 ...\n",
    "   |   |   |   - images/\n",
    "   |   |   |   |   id.jpg\n",
    "                 ...\n",
    "   |   |   - dense1/\n",
    "            ...\n",
    "      ...\n",
    "```\n",
    "\n",
    "The directory structure needs to be the same as shown, even if the data being used is different. To train on different data, create the same directory structure and with data files in the same format. The folder marked 0000/ above represents that everything inside is a part of the same \"scene\". In the case of the included dataset, images of the same building from different angles/lightings/etc are considered part of the same scene. The format for pairs.txt is shown above, and is needed to add the additional info about pairs of images that OpenGlue uses in it's training. OpenGlue does not include code to generate these files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08642df5-2b4c-42d2-aa57-b6188d8f3e5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Take a Subset of the Data\n",
    "OpenGlue includes files specifying which scenes (as in the folder names which each contain a single scene) are to be used for training, testing, and validation, but does not include code for generating new versions of these or for making a smaller sample set.\n",
    "To take a subset of the data and only use a small amount in training and testing, we need to generate new versions of these files. However, OpenGlue has an intermediate step that is reccommended to be done prior to training, feature extraction, which does not utilize these files and extracts features (keypoints) from the images in the dataset. Feature extraction is highly expensive in terms of disk usage, and takes a long time to do, but speeds up training time. There is not a method implemented in OpenGlue to only preform feature extraction on a subset of the data, but alternatively, we can skip feature extraction as OpenGlue has 2 training methods, one which relies on feature extraction being done prior, and one which does it as training is done. By utilizing the latter, we can have OpenGlue preform feature extraction and training on only a subset of the data, since, although training time would be fastest with a subset and with feature extraction done prior, doing this will make the overall time to go from the beginning of the pipeline to the end faster.\n",
    "\n",
    "Additionally, store test data in a seperate folder. OpenGlue tests via running inference on 2 jpg images. The images to test on are not provided by OpenGlue, but you can use kaggle data to test (https://www.kaggle.com/competitions/image-matching-challenge-2022/data?select=test_images) or use other images that are applicable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b26acd18-e92b-4e39-81ac-9da5ff00f27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Generate files for train/validate split\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import random\n",
    "def split_data(root_data_dir, percent_used, validate_set_percent, split_config_dir, split_config_name):\n",
    "    all_scenes = os.listdir(root_data_dir)\n",
    "    sub_set = [scene for scene in all_scenes if random.random() <= percent_used] \n",
    "    print(sub_set)\n",
    "    \n",
    "    train_set_size = int(len(sub_set) * (1-validate_set_percent))\n",
    "    valid_set_size = len(sub_set) - train_set_size\n",
    "    \n",
    "    seed = torch.Generator().manual_seed(42)\n",
    "    train_set, valid_set = data.random_split(sub_set, [train_set_size, valid_set_size], generator=seed)\n",
    "    print(train_set)\n",
    "     \n",
    "    sub_set_path = os.path.join(split_config_dir, split_config_name+'-total.txt')\n",
    "    train_set_path = os.path.join(split_config_dir, split_config_name+'-train.txt')\n",
    "    val_set_path = os.path.join(split_config_dir, split_config_name+'-val.txt')\n",
    "    \n",
    "    with open(sub_set_path, 'w') as fout:\n",
    "        for scene in sub_set:\n",
    "            fout.write(scene+'\\n')\n",
    "    with open(train_set_path, 'w') as fout:\n",
    "        for scene in train_set:\n",
    "            fout.write(scene+'\\n')\n",
    "    with open(val_set_path, 'w') as fout:\n",
    "        for scene in valid_set:\n",
    "            fout.write(scene+'\\n')\n",
    "    return train_set_path, val_set_path, sub_set_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b1ea0b-ccd2-4018-9c63-95a3de95bd24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0032', '0047', '0223', '0252']\n",
      "<torch.utils.data.dataset.Subset object at 0x7fbc9a7cccd0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./assets/subset-0025-02-train.txt',\n",
       " './assets/subset-0025-02-val.txt',\n",
       " './assets/subset-0025-02-total.txt')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data('/host_Data/Data/MegaDepth/MegaDepth/phoenix/S6/zl548/MegaDepth_v1', 0.025, 0.2, './assets/', 'subset-0025-02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189db336-5982-4fb6-9fae-f6138a93affd",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "You can run feature extraction as a separate step, prior to training. This will make training run faster, but is very expensive in terms of disk space and time, as there are not any built in options inside of the OpenGlue framework to preform feature extraction on a subset of the data. The only ways to do this are to either manually move all of the scene folders not included in the files specifying the subsets generated in the previous step to a different location, or to not run feature extraction as a separate step and instead run OpenGlue training such that feature extraction is done concurrently with training, meaning only the scene files used for training/validation will have feature extraction preformed on them.\n",
    "<br />The steps to do feature extraction as a separate step are detailed in the README:<br />\n",
    "run `python extract_features.py` with the following parameters (for more details, please, refer to module's documentation):\n",
    "      \n",
    "   * `--device` - `[cpu, cuda]`\n",
    "   * `--num_workers` - number of workers for parallel processing. When cpu is chosen, assigns the exact amount of the set workers, for gpu scenario it takes into account the number of gpu units available. \n",
    "   * `--target_size` - target size of the image (WIDTH, HEIGHT).\n",
    "   * `--data_path` - path to directory with scenes images\n",
    "   * `--output_path` - path to directory where extracted features are stored\n",
    "   * `--extractor_config_path` - 'path to the file containing config for feature extractor in .yaml format\n",
    "   * `--recompute` - include this flag to recompute features if it is already present in output directory\n",
    "   * `--image_format` - formats of images searched inside `data_path` to compute features for, default: [jpg, JPEG, JPG, png]\n",
    "   \n",
    "   Choosing local feature extractor is performed via `--extractor_config_path`. \n",
    "   We provide configs in `config/features/`, which user can edit or use unchanged:\n",
    "   * SuperPoint with MagicLeap weights - [`superpoint_magicleap.yaml`](config/features/superpoint_magicleap.yaml)\n",
    "   * SuperPoint with KITTI weights - [`superpoint_kitti.yaml`](config/features/superpoint_kitti.yaml)\n",
    "   * SuperPoint with COCO weights - [`superpoint_coco.yaml`](config/features/superpoint_coco.yaml)\n",
    "   * SIFT opencv - [`sift_opencv.yaml`](config/features/sift_opencv.yaml)\n",
    "   * DoG-AffNet-HardNet - [`dog_opencv_affnet_hardnet.yaml`](config/features/dog_opencv_affnet_hardnet.yaml)\n",
    "<br />I have included an example command to run this from the command line, as well as reimplemented the necessary functions such that it can be run from here, with the arguments being in a list rather than entered as command line arguments. The command line version would look like:<br /> `python extract_features.py --device 'cuda' --num_workers '1' --target_size 1280 1280 --data_path '/host_Data/Data/MegaDepth/MegaDepth/phoenix/S6/zl548/MegaDepth_v1/' --output_path '/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix' --extractor_config_path 'config/features/superpoint_magicleap.yaml'`<br />\n",
    "Note: if feature extraction is run as a separate step, be sure to take note of the target size you use. In this case it is 1280 by 1280, which is what I used when I ran feature extraction on it's own, but this value needs to be consistent inside the config files to be used during training. Also, this value could be made smaller, which may improve runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0eb33e9-f643-438c-b30b-4789384bf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From: extract_features.py\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Tuple, List, Union, Optional\n",
    "\n",
    "import cv2\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from torch import multiprocessing\n",
    "\n",
    "from models.features import get_feature_extractor\n",
    "\n",
    "import extract_features\n",
    "\n",
    "def parse_arguments(arguments):\n",
    "    \"\"\"\n",
    "    Define and parse command line arguments for this module.\n",
    "    Returns:\n",
    "        args: Namespace object containing parsed arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser('Local Features Extraction')\n",
    "    parser.add_argument(\n",
    "        '--device',\n",
    "        help='Device type, where features are extracted',\n",
    "        type=str,\n",
    "        default='cpu',\n",
    "        choices=['cpu', 'cuda'],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_workers',\n",
    "        help='Number of workers for parallel processing',\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--target_size',\n",
    "        help='Target size of the image (WIDTH, HEIGHT). '\n",
    "             'At least one side of resulting image will correspond with `target_size` '\n",
    "             'dimensions, such that the original aspect ration is preserved',\n",
    "        type=int,\n",
    "        default=None,\n",
    "        nargs=2\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--data_path',\n",
    "        help='Path to directory with scenes images',\n",
    "        type=pathlib.Path,\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_path',\n",
    "        help='Path to directory where extracted features are stored',\n",
    "        type=pathlib.Path,\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--extractor_config_path',\n",
    "        help='Path to the file containing config for feature extractor in .yaml format',\n",
    "        type=pathlib.Path,\n",
    "        default='config/features/sift_opencv.yaml'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--recompute',\n",
    "        help='Flag indicating whether to recompute features if it is already present in output directory',\n",
    "        action='store_true'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--image_format',\n",
    "        help='Formats of images searched inside `data_path` to compute features for',\n",
    "        type=str,\n",
    "        default=['jpg', 'JPEG', 'JPG', 'png'],\n",
    "        nargs='+'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--scene_subset_path',\n",
    "        help='Path to the file containing the list of scenes to preform feature extraction on within the root directory. All scenes proccessed if unset.',\n",
    "        type=pathlib.Path,\n",
    "        default='./None'\n",
    "    )\n",
    "    return parser.parse_args(arguments)\n",
    "\n",
    "def get_images_list(input_data_path: pathlib.Path, image_formats: List[str], scene_subset_path: pathlib.Path) -> List[Tuple[str, Optional[str]]]:\n",
    "    \"\"\"\n",
    "    Get list of images that wil be processed.\n",
    "    Args:\n",
    "        input_data_path: input path to the location with images\n",
    "        image_formats: file formats of images to look for\n",
    "        scence_subset_path: input path to file specifying which scenes to use. All scenes used if set to ./None\n",
    "    Returns:\n",
    "        images_path_list: list where each image is represented as tuple with path to image\n",
    "        and scene name (or None if no scenes are available)\n",
    "    \"\"\"\n",
    "    \n",
    "    scene_set = []\n",
    "    if scene_subset_path != \"./None\":\n",
    "        with open(scene_subset_path, 'r') as fin:\n",
    "            scene_set = fin.read().split('\\n')\n",
    "    print(scene_set)\n",
    "    # process each scene\n",
    "    images_path_list = []\n",
    "    \n",
    "    #Added to take subset prior to extraction\n",
    "    scenes = os.listdir(input_data_path)\n",
    "    for scene in scenes:\n",
    "        if scene not in scene_set:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        images_path = input_data_path / scene / 'dense0' / 'imgs'\n",
    "        scene_list = []\n",
    "        for image_format in image_formats:\n",
    "            scene_list.extend(glob.glob(str(images_path / f'*.{image_format}')))\n",
    "        images_path_list.extend(((path, scene) for path in scene_list))\n",
    "    return images_path_list\n",
    "\n",
    "def extract(arguments):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    args = parse_arguments(arguments)\n",
    "    logger.info(args)\n",
    "\n",
    "    num_workers = args.num_workers\n",
    "    if args.device == 'cuda' and torch.cuda.device_count() < num_workers:\n",
    "        logger.warning(f'Number of workers selected is bigger than number of available cuda devices. '\n",
    "                       f'Setting num_workers to {torch.cuda.device_count()}.')\n",
    "        num_workers = torch.cuda.device_count()\n",
    "\n",
    "    # read feature extractor config\n",
    "    with open(args.extractor_config_path) as f:\n",
    "        feature_extractor_config = yaml.full_load(f)\n",
    "\n",
    "    # make output directory\n",
    "    output_path = args.output_path / extract_features.get_output_directory_name(feature_extractor_config, args)\n",
    "    logger.info(f'Creating output directory {output_path} (if not exists).')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with open(os.path.join(output_path, 'config.yaml'), 'w') as f:\n",
    "        yaml.dump(feature_extractor_config, f)\n",
    "\n",
    "\n",
    "    \n",
    "    images_list = get_images_list(args.data_path, args.image_format, args.scene_subset_path)\n",
    "    logger.info(f'Total number of images found to process: {len(images_list)}')\n",
    "    # split into chunks of (almost) equal size\n",
    "    chunk_size = math.ceil(len(images_list) / num_workers)\n",
    "    images_list = [images_list[i * chunk_size:(i + 1) * chunk_size] for i in range(num_workers)]\n",
    "\n",
    "    logger.info(f'Starting {num_workers} processes for features extraction.')\n",
    "    multiprocessing.start_processes(\n",
    "        extract_features.process_chunk,\n",
    "        args=(images_list, feature_extractor_config, output_path, args),\n",
    "        nprocs=num_workers,\n",
    "        join=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d71017-38c3-4260-8300-71aa56eebc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/06/13 11:14:54] __main__ | INFO: Namespace(data_path=PosixPath('/host_Data/Data/MegaDepth/MegaDepth/phoenix/S6/zl548/MegaDepth_v1'), device='cuda', extractor_config_path=PosixPath('config/features/superpoint_magicleap.yaml'), image_format=['jpg', 'JPEG', 'JPG', 'png'], num_workers=1, output_path=PosixPath('/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix2'), recompute=False, scene_subset_path=PosixPath('assets/subset-20-20-total.txt'), target_size=[960, 720])\n",
      "[2022/06/13 11:14:54] __main__ | INFO: Creating output directory /host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix2/SuperPointNet_960_720 (if not exists).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0024', '0048', '0102', '0148', '0181', '0307', '']\n",
      "['0000', '0001', '0002', '0003', '0004', '0005', '0007', '0008', '0011', '0012', '0013', '0015', '0016', '0017', '0019', '0020', '0021', '0022', '0023', '0024', '0025', '0026', '0027', '0032', '0033', '0034', '0035', '0036', '0037', '0039', '0041', '0042', '0043', '0044', '0046', '0047', '0048', '0049', '0050', '0056', '0057', '0058', '0060', '0061', '0062', '0063', '0064', '0065', '0067', '0070', '0071', '0076', '0078', '0080', '0083', '0086', '0087', '0090', '0092', '0094', '0095', '0098', '0099', '0100', '0101', '0102', '0103', '0104', '0105', '0107', '0115', '0117', '0121', '0122', '0129', '0130', '0133', '0137', '0141', '0143', '0147', '0148', '0149', '0150', '0151', '0156', '0160', '0162', '0168', '0175', '0176', '0177', '0178', '0181', '0183', '0185', '0186', '0189', '0190', '0197', '0200', '0204', '0205', '0209', '0212', '0214', '0217', '0223', '0224', '0229', '0231', '0235', '0237', '0238', '0240', '0243', '0252', '0257', '0258', '0265', '0269', '0271', '0275', '0277', '0281', '0285', '0286', '0290', '0294', '0299', '0303', '0306', '0307', '0312', '0323', '0326', '0327', '0331', '0335', '0341', '0348', '0349', '0360', '0366', '0377', '0380', '0387', '0389', '0394', '0402', '0406', '0407', '0411', '0412', '0430', '0443', '0446', '0455', '0472', '0474', '0476', '0478', '0482', '0493', '0494', '0496', '0505', '0559', '0733', '0768', '0860', '1001', '1017', '1589', '3346', '4541', '5000', '5001', '5002', '5003', '5004', '5005', '5006', '5007', '5008', '5009', '5010', '5011', '5012', '5013', '5014', '5015', '5016', '5017', '5018'] ['0024', '0048', '0102', '0148', '0181', '0307', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/06/13 11:14:55] __main__ | INFO: Total number of images found to process: 2147\n",
      "[2022/06/13 11:14:55] __main__ | INFO: Starting 1 processes for features extraction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/06/13 11:15:34] extract_features | INFO: PID #0: Processed 100/2147 images.\n"
     ]
    }
   ],
   "source": [
    "args = ['--device', 'cuda', '--num_workers', '1', '--target_size', '960', '720', '--data_path', '/host_Data/Data/MegaDepth/MegaDepth/phoenix/S6/zl548/MegaDepth_v1/', '--output_path', '/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3', '--extractor_config_path', './config/features/superpoint_magicleap_1024.yaml', '--scene_subset_path', './assets/subset-20-20-total.txt']\n",
    "extract(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201fde2e-323a-4871-a9e6-8cf89bbfe724",
   "metadata": {},
   "source": [
    "## Training\n",
    "Before training, set all necessary hyperparameters configurations in your config file.\n",
    "If you selected to extract features during training, then edit [`config/config.yaml`](config/config.yaml). For pre-extracted features `config/config_cached.yaml` will be used.\n",
    "\n",
    "\n",
    "The description of possible configurations for training is provided in [`CONFIGURATIONS.md`](CONFIGURATIONS.md)\n",
    "\n",
    "\n",
    "Be aware, that default configs for feature extraction set a maximum of 1024 keypoints per image. For second option - pre-extraction, more keypoints can be detected and saved.\n",
    "When calculating features during training, one of the corresponding configs from [`config/features_online`](config/features_online) should be selected and forwarded as a parameter, see example below. So, there is a great difference between launching with config.yaml or config_cached.yaml .\n",
    "\n",
    "\n",
    "<b>To launch train with local feature extraction throughout training, run: </b>  \n",
    "```\n",
    "python train.py --config='config/config.yaml' --features_config='config/features_online/sift.yaml'\n",
    "```\n",
    "sift.yaml is an example, can be any config file from `config/features_online`. <i>Important Note: </i> DoG-AffNet-HardNet is only available for cached features run. We do not recommend extracting features with this model during training.\n",
    "\n",
    "\n",
    "<b>To launch train with cached local features, run: </b>   \n",
    "```\n",
    "python train_cached.py --config='config/config_cached.yaml'\n",
    "```\n",
    "The parameter responsible for configuring cached features location is `features_dir`.\n",
    "\n",
    "\n",
    "The logging results will be visible inside a log folder + experiment name, specified in `config.yaml`. <br />\n",
    "\n",
    "Given the plugins and training settings that OpenGlue uses, specifically the accelerator and strategy settings, training cannot be run inside of a jupyter notebook. Run the given command, depending on whether you are using pre-extracted features or not, from the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bcea3-7249-49f4-ba65-1614d9230c88",
   "metadata": {},
   "source": [
    "## Training with pre-extraction (cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d807eb-ab21-499e-bcac-bcefcb263812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code/OpenGlue/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "import torch\n",
    "import shutup\n",
    "\n",
    "shutup.please()\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from omegaconf import OmegaConf\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DataParallelStrategy\n",
    "\n",
    "from data.megadepth_datamodule import MegaDepthPairsDataModuleFeatures\n",
    "from models.matching_module import MatchingTrainingModule\n",
    "from utils.train_utils import get_training_loggers, get_training_callbacks, prepare_logging_directory\n",
    "\n",
    "\n",
    "def train_cached(config_path='config/config_cached.yaml'):\n",
    "\n",
    "    # Load config\n",
    "    config = OmegaConf.load('config/config_cached.yaml')  # base config\n",
    "    if config_path != 'config/config_cached.yaml':\n",
    "        add_conf = OmegaConf.load(config_path)\n",
    "        config = OmegaConf.merge(config, add_conf)\n",
    "\n",
    "    pl.seed_everything(int(os.environ.get('LOCAL_RANK', 0)))\n",
    "\n",
    "    # Prepare directory for logs and checkpoints\n",
    "    if os.environ.get('LOCAL_RANK', 0) == 0:\n",
    "        experiment_name = '{}_cache__attn_{}__laf_{}__{}'.format(\n",
    "            config['data']['features_dir'],\n",
    "            config['superglue']['attention_gnn']['attention'],\n",
    "            config['superglue']['laf_to_sideinfo_method'],\n",
    "            str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        )\n",
    "        log_path = prepare_logging_directory(config, experiment_name)\n",
    "    else:\n",
    "        experiment_name, log_path = '', ''\n",
    "\n",
    "    # Init Lightning Data Module\n",
    "    data_config = config['data']\n",
    "    dm = MegaDepthPairsDataModuleFeatures(\n",
    "        root_path=data_config['root_path'],\n",
    "        train_list_path=data_config['train_list_path'],\n",
    "        val_list_path=data_config['val_list_path'],\n",
    "        test_list_path=data_config['test_list_path'],\n",
    "        batch_size=data_config['batch_size_per_gpu'],\n",
    "        num_workers=data_config['dataloader_workers_per_gpu'],\n",
    "        target_size=data_config['target_size'],\n",
    "        features_dir=data_config['features_dir'],\n",
    "        num_keypoints=data_config['max_keypoints'],\n",
    "        val_max_pairs_per_scene=data_config['val_max_pairs_per_scene'],\n",
    "        balanced_train=data_config.get('balanced_train', False),\n",
    "        train_pairs_overlap=data_config.get('train_pairs_overlap')\n",
    "    )\n",
    "\n",
    "    features_config = OmegaConf.load(os.path.join(config['data']['root_path'],\n",
    "                                                  config['data']['features_dir'], 'config.yaml'))\n",
    "\n",
    "    # Init model\n",
    "    model = MatchingTrainingModule(\n",
    "        train_config={**config['train'], **config['inference'], **config['evaluation']},\n",
    "        features_config=features_config,\n",
    "        superglue_config=config['superglue'],\n",
    "    )\n",
    "\n",
    "    # Set callbacks and loggers\n",
    "    callbacks = get_training_callbacks(config, log_path, experiment_name)\n",
    "    loggers = get_training_loggers(config, log_path, experiment_name)\n",
    "\n",
    "    # Init trainer\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=config['gpus'],\n",
    "        max_epochs=config['train']['epochs'],\n",
    "        accelerator=\"gpu\",\n",
    "        gradient_clip_val=config['train']['grad_clip'],\n",
    "        log_every_n_steps=config['logging']['train_logs_steps'],\n",
    "        limit_train_batches=config['train']['steps_per_epoch'],\n",
    "        num_sanity_val_steps=5,\n",
    "        callbacks=callbacks,\n",
    "        logger=loggers,\n",
    "        strategy=DataParallelStrategy(),\n",
    "        #plugins=DDPPlugin(find_unused_parameters=False),\n",
    "        precision=config['train'].get('precision', 32),\n",
    "    )\n",
    "    # If loaded from checkpoint - validate\n",
    "    if config.get('checkpoint') is not None:\n",
    "        trainer.validate(model, datamodule=dm, ckpt_path=config.get('checkpoint'))\n",
    "    trainer.fit(model, datamodule=dm, ckpt_path=config.get('checkpoint'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ec4ac3-6dd3-4139-8566-5dac00bf7adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Feb_14_21:12:58_PST_2021\n",
      "Cuda compilation tools, release 11.2, V11.2.152\n",
      "Build cuda_11.2.r11.2/compiler.29618528_0\n",
      "Python 3.8.10\n",
      "11030\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "!nvcc --version\n",
    "!python --version\n",
    "\n",
    "print(torch._C._cuda_getCompiledVersion())\n",
    "print(torch._C._cuda_getDevice())\n",
    "print(torch._C._cuda_getDeviceCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3ec5c-f8ab-4533-8608-5c19dae3fd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlunder\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/code/OpenGlue/wandb/run-20220614_224356-e41wo6qt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jlunder/superglue/runs/e41wo6qt\" target=\"_blank\">/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3/SuperPointNet_960_720_cache__attn_softmax__laf_none__2022-06-14-22-43-54</a></strong> to <a href=\"https://wandb.ai/jlunder/superglue\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type                      | Params\n",
      "---------------------------------------------------------------------\n",
      "0 | superglue              | SuperGlue                 | 12.0 M\n",
      "1 | augmentations          | AugmentationSequential    | 0     \n",
      "2 | epipolar_dist_metric   | AccuracyUsingEpipolarDist | 0     \n",
      "3 | camera_pose_auc_metric | CameraPoseAUC             | 0     \n",
      "---------------------------------------------------------------------\n",
      "12.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.0 M    Total params\n",
      "47.829    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]correction?\n",
      "correction?\n",
      "correction?\n",
      "Sanity Checking DataLoader 0:  20%|██        | 1/5 [00:00<00:00, 11.81it/s]correction?\n",
      "correction?\n",
      "correction?\n",
      "Sanity Checking DataLoader 0:  40%|████      | 2/5 [00:00<00:01,  2.20it/s]correction?\n",
      "correction?\n",
      "correction?\n",
      "Sanity Checking DataLoader 0:  60%|██████    | 3/5 [00:01<00:01,  1.53it/s]correction?\n",
      "correction?\n",
      "correction?\n",
      "Sanity Checking DataLoader 0:  80%|████████  | 4/5 [00:02<00:00,  1.29it/s]correction?\n",
      "correction?\n",
      "correction?\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 5/5 [00:03<00:00,  1.27it/s]correction?\n",
      "correction?\n",
      "correction?\n",
      "Epoch 0:  91%|█████████ | 500/550 [03:34<00:21,  2.34it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[Acorrection?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Epoch 0:  91%|█████████ | 501/550 [03:34<00:21,  2.33it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:   4%|▍         | 2/50 [00:00<00:14,  3.34it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████▏| 502/550 [03:35<00:20,  2.33it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:   6%|▌         | 3/50 [00:01<00:20,  2.28it/s]\u001b[A\n",
      "Epoch 0:  91%|█████████▏| 503/550 [03:35<00:20,  2.33it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:   8%|▊         | 4/50 [00:02<00:27,  1.70it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 504/550 [03:36<00:19,  2.32it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  10%|█         | 5/50 [00:03<00:32,  1.40it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 505/550 [03:37<00:19,  2.32it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  12%|█▏        | 6/50 [00:03<00:32,  1.34it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 506/550 [03:38<00:19,  2.32it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  14%|█▍        | 7/50 [00:04<00:33,  1.30it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 507/550 [03:39<00:18,  2.31it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  16%|█▌        | 8/50 [00:05<00:37,  1.13it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 508/550 [03:40<00:18,  2.30it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  18%|█▊        | 9/50 [00:06<00:36,  1.13it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 509/550 [03:41<00:17,  2.30it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  20%|██        | 10/50 [00:07<00:33,  1.19it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 510/550 [03:42<00:17,  2.30it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  22%|██▏       | 11/50 [00:08<00:32,  1.18it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 511/550 [03:42<00:17,  2.29it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  24%|██▍       | 12/50 [00:09<00:33,  1.13it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 512/550 [03:43<00:16,  2.29it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  26%|██▌       | 13/50 [00:09<00:30,  1.23it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 513/550 [03:44<00:16,  2.28it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  28%|██▊       | 14/50 [00:10<00:29,  1.21it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 514/550 [03:45<00:15,  2.28it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  30%|███       | 15/50 [00:11<00:29,  1.20it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▎| 515/550 [03:46<00:15,  2.28it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  32%|███▏      | 16/50 [00:12<00:27,  1.22it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 516/550 [03:47<00:14,  2.27it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  34%|███▍      | 17/50 [00:13<00:26,  1.25it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 517/550 [03:47<00:14,  2.27it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  36%|███▌      | 18/50 [00:13<00:24,  1.32it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 518/550 [03:48<00:14,  2.27it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  38%|███▊      | 19/50 [00:14<00:23,  1.30it/s]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 519/550 [03:49<00:13,  2.26it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  40%|████      | 20/50 [00:15<00:22,  1.31it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 520/550 [03:50<00:13,  2.26it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  42%|████▏     | 21/50 [00:16<00:22,  1.26it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 521/550 [03:50<00:12,  2.26it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  44%|████▍     | 22/50 [00:17<00:23,  1.20it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 522/550 [03:51<00:12,  2.25it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  46%|████▌     | 23/50 [00:17<00:20,  1.33it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 523/550 [03:52<00:11,  2.25it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  48%|████▊     | 24/50 [00:18<00:20,  1.25it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 524/550 [03:53<00:11,  2.25it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  50%|█████     | 25/50 [00:19<00:19,  1.28it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 525/550 [03:54<00:11,  2.24it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  52%|█████▏    | 26/50 [00:20<00:20,  1.17it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 526/550 [03:55<00:10,  2.24it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  54%|█████▍    | 27/50 [00:21<00:21,  1.09it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 527/550 [03:56<00:10,  2.23it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  56%|█████▌    | 28/50 [00:22<00:19,  1.13it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 528/550 [03:56<00:09,  2.23it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  58%|█████▊    | 29/50 [00:23<00:17,  1.17it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 529/550 [03:57<00:09,  2.23it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  60%|██████    | 30/50 [00:24<00:18,  1.09it/s]\u001b[A\n",
      "Epoch 0:  96%|█████████▋| 530/550 [03:58<00:09,  2.22it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  62%|██████▏   | 31/50 [00:25<00:18,  1.03it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 531/550 [03:59<00:08,  2.21it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  64%|██████▍   | 32/50 [00:26<00:18,  1.01s/it]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 532/550 [04:01<00:08,  2.21it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  66%|██████▌   | 33/50 [00:27<00:15,  1.07it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 533/550 [04:01<00:07,  2.20it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  68%|██████▊   | 34/50 [00:27<00:14,  1.11it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 534/550 [04:02<00:07,  2.20it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  70%|███████   | 35/50 [00:28<00:13,  1.13it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 535/550 [04:03<00:06,  2.20it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  72%|███████▏  | 36/50 [00:29<00:12,  1.10it/s]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 536/550 [04:04<00:06,  2.19it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  74%|███████▍  | 37/50 [00:30<00:10,  1.22it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 537/550 [04:05<00:05,  2.19it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  76%|███████▌  | 38/50 [00:31<00:10,  1.13it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 538/550 [04:06<00:05,  2.19it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  78%|███████▊  | 39/50 [00:32<00:10,  1.06it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 539/550 [04:07<00:05,  2.18it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  80%|████████  | 40/50 [00:33<00:09,  1.10it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 540/550 [04:07<00:04,  2.18it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  82%|████████▏ | 41/50 [00:34<00:07,  1.13it/s]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 541/550 [04:08<00:04,  2.17it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  84%|████████▍ | 42/50 [00:35<00:07,  1.10it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▊| 542/550 [04:09<00:03,  2.17it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  86%|████████▌ | 43/50 [00:35<00:06,  1.15it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▊| 543/550 [04:10<00:03,  2.17it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  88%|████████▊ | 44/50 [00:36<00:05,  1.16it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 544/550 [04:11<00:02,  2.16it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 45/50 [00:37<00:04,  1.18it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 545/550 [04:12<00:02,  2.16it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  92%|█████████▏| 46/50 [00:38<00:03,  1.19it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 546/550 [04:13<00:01,  2.16it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  94%|█████████▍| 47/50 [00:39<00:02,  1.11it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 547/550 [04:14<00:01,  2.15it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  96%|█████████▌| 48/50 [00:40<00:01,  1.09it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 548/550 [04:15<00:00,  2.15it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0:  98%|█████████▊| 49/50 [00:41<00:00,  1.13it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 549/550 [04:15<00:00,  2.15it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "\n",
      "Validation DataLoader 0: 100%|██████████| 50/50 [00:41<00:00,  1.17it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 550/550 [04:16<00:00,  2.14it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]correction?\n",
      "correction?\n",
      "correction?\n",
      "Epoch 0: 100%|██████████| 550/550 [04:16<00:00,  2.14it/s, loss=3.44, v_num=o6qt, Train NLL loss=2.670, Train Metric loss=0.000]\n",
      "Epoch 1:  11%|█         | 60/550 [00:26<03:38,  2.24it/s, loss=4.67, v_num=o6qt, Train NLL loss=4.910, Train Metric loss=0.000] "
     ]
    }
   ],
   "source": [
    "train_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a82cd8-c50d-4490-b45e-1643a5207f68",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56738b7e-dac7-4a92-a358-7380bf314dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import inference\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Where the images to run inference on are\n",
    "img0_path = '/host_Data/Data/MegaDepth/MegaDepth/test_images/img2.jpg'\n",
    "img1_path = '/host_Data/Data/MegaDepth/MegaDepth/test_images/img3.jpg'\n",
    "\n",
    "#Where the configurations and checkpoints saved during training are\n",
    "experiment_path = '/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3/SuperPointNet_960_720_cache__attn_softmax__laf_none__2022-06-14-21-21-28'\n",
    "checkpoint_name = 'superglue-step=1000.ckpt'\n",
    "\n",
    "#which device to use to run inference (gpu or cpu)\n",
    "device = 'cuda:0'\n",
    "\n",
    "#output location\n",
    "output_dir = 'results.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a690580-f969-4ab2-bbf9-b97e8541684c",
   "metadata": {},
   "source": [
    "- Config.yaml in 'experiment_path' directory\n",
    "    - Under superglue:\n",
    "        - Add 'descriptor_dim: <dim>' where <dim> is the same as the corresponding field in features_config.yaml in the same folder (usually 256)\n",
    "        - Under positional_encoding:\n",
    "            - Add and set 'output_size: ' to the same value\n",
    "        - Under attention_gnn:\n",
    "            - Add and set 'embed_dim: ' to the same value\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe090d1-ac3f-4456-8590-c5d203dd0175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'descriptor_dim': 256, 'name': 'SuperPointNet', 'parameters': {'descriptor_dim': 256, 'keypoint_threshold': 0.005, 'max_keypoints': 1024, 'nms_kernel': 9, 'remove_borders_size': 4, 'weights': './models/superglue/weights/superpoint_magicleap_v1.pth'}, 'max_keypoints': 1024}\n",
      "<All keys matched successfully>\n",
      "superglue.mix_coefs\n",
      "superglue.dustbin_score\n",
      "superglue.positional_encoding.encoder.0.weight\n",
      "superglue.positional_encoding.encoder.0.bias\n",
      "superglue.positional_encoding.encoder.2.weight\n",
      "superglue.positional_encoding.encoder.2.bias\n",
      "superglue.positional_encoding.encoder.2.running_mean\n",
      "superglue.positional_encoding.encoder.2.running_var\n",
      "superglue.positional_encoding.encoder.2.num_batches_tracked\n",
      "superglue.positional_encoding.encoder.3.weight\n",
      "superglue.positional_encoding.encoder.3.bias\n",
      "superglue.positional_encoding.encoder.5.weight\n",
      "superglue.positional_encoding.encoder.5.bias\n",
      "superglue.positional_encoding.encoder.5.running_mean\n",
      "superglue.positional_encoding.encoder.5.running_var\n",
      "superglue.positional_encoding.encoder.5.num_batches_tracked\n",
      "superglue.positional_encoding.encoder.6.weight\n",
      "superglue.positional_encoding.encoder.6.bias\n",
      "superglue.positional_encoding.encoder.8.weight\n",
      "superglue.positional_encoding.encoder.8.bias\n",
      "superglue.positional_encoding.encoder.8.running_mean\n",
      "superglue.positional_encoding.encoder.8.running_var\n",
      "superglue.positional_encoding.encoder.8.num_batches_tracked\n",
      "superglue.positional_encoding.encoder.9.weight\n",
      "superglue.positional_encoding.encoder.9.bias\n",
      "superglue.attention_gnn.layers.0.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.0.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.0.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.0.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.0.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.0.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.0.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.0.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.0.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.0.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.0.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.0.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.0.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.0.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.0.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.0.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.0.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.1.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.1.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.1.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.1.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.1.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.1.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.1.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.1.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.1.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.1.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.1.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.1.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.1.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.1.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.1.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.1.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.1.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.2.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.2.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.2.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.2.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.2.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.2.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.2.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.2.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.2.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.2.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.2.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.2.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.2.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.2.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.2.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.2.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.2.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.3.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.3.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.3.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.3.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.3.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.3.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.3.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.3.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.3.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.3.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.3.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.3.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.3.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.3.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.3.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.3.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.3.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.4.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.4.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.4.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.4.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.4.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.4.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.4.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.4.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.4.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.4.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.4.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.4.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.4.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.4.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.4.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.4.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.4.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.5.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.5.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.5.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.5.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.5.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.5.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.5.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.5.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.5.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.5.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.5.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.5.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.5.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.5.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.5.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.5.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.5.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.6.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.6.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.6.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.6.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.6.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.6.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.6.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.6.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.6.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.6.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.6.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.6.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.6.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.6.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.6.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.6.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.6.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.7.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.7.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.7.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.7.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.7.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.7.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.7.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.7.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.7.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.7.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.7.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.7.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.7.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.7.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.7.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.7.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.7.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.8.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.8.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.8.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.8.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.8.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.8.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.8.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.8.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.8.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.8.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.8.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.8.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.8.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.8.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.8.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.8.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.8.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.9.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.9.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.9.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.9.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.9.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.9.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.9.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.9.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.9.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.9.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.9.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.9.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.9.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.9.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.9.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.9.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.9.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.10.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.10.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.10.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.10.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.10.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.10.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.10.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.10.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.10.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.10.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.10.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.10.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.10.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.10.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.10.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.10.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.10.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.11.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.11.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.11.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.11.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.11.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.11.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.11.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.11.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.11.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.11.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.11.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.11.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.11.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.11.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.11.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.11.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.11.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.12.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.12.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.12.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.12.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.12.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.12.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.12.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.12.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.12.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.12.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.12.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.12.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.12.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.12.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.12.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.12.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.12.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.13.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.13.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.13.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.13.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.13.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.13.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.13.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.13.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.13.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.13.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.13.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.13.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.13.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.13.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.13.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.13.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.13.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.14.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.14.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.14.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.14.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.14.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.14.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.14.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.14.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.14.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.14.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.14.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.14.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.14.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.14.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.14.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.14.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.14.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.15.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.15.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.15.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.15.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.15.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.15.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.15.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.15.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.15.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.15.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.15.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.15.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.15.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.15.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.15.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.15.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.15.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.16.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.16.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.16.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.16.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.16.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.16.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.16.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.16.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.16.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.16.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.16.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.16.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.16.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.16.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.16.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.16.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.16.module.fc.3.bias\n",
      "superglue.attention_gnn.layers.17.module.mha.in_proj_q.weight\n",
      "superglue.attention_gnn.layers.17.module.mha.in_proj_q.bias\n",
      "superglue.attention_gnn.layers.17.module.mha.in_proj_k.weight\n",
      "superglue.attention_gnn.layers.17.module.mha.in_proj_k.bias\n",
      "superglue.attention_gnn.layers.17.module.mha.in_proj_v.weight\n",
      "superglue.attention_gnn.layers.17.module.mha.in_proj_v.bias\n",
      "superglue.attention_gnn.layers.17.module.mha.out_proj.weight\n",
      "superglue.attention_gnn.layers.17.module.mha.out_proj.bias\n",
      "superglue.attention_gnn.layers.17.module.fc.0.weight\n",
      "superglue.attention_gnn.layers.17.module.fc.0.bias\n",
      "superglue.attention_gnn.layers.17.module.fc.2.weight\n",
      "superglue.attention_gnn.layers.17.module.fc.2.bias\n",
      "superglue.attention_gnn.layers.17.module.fc.2.running_mean\n",
      "superglue.attention_gnn.layers.17.module.fc.2.running_var\n",
      "superglue.attention_gnn.layers.17.module.fc.2.num_batches_tracked\n",
      "superglue.attention_gnn.layers.17.module.fc.3.weight\n",
      "superglue.attention_gnn.layers.17.module.fc.3.bias\n",
      "superglue.linear_proj.weight\n",
      "superglue.linear_proj.bias\n",
      "mix_coefs \t torch.Size([256, 1])\n",
      "dustbin_score \t torch.Size([])\n",
      "positional_encoding.encoder.0.weight \t torch.Size([32, 3, 1])\n",
      "positional_encoding.encoder.0.bias \t torch.Size([32])\n",
      "positional_encoding.encoder.2.weight \t torch.Size([32])\n",
      "positional_encoding.encoder.2.bias \t torch.Size([32])\n",
      "positional_encoding.encoder.2.running_mean \t torch.Size([32])\n",
      "positional_encoding.encoder.2.running_var \t torch.Size([32])\n",
      "positional_encoding.encoder.2.num_batches_tracked \t torch.Size([])\n",
      "positional_encoding.encoder.3.weight \t torch.Size([64, 32, 1])\n",
      "positional_encoding.encoder.3.bias \t torch.Size([64])\n",
      "positional_encoding.encoder.5.weight \t torch.Size([64])\n",
      "positional_encoding.encoder.5.bias \t torch.Size([64])\n",
      "positional_encoding.encoder.5.running_mean \t torch.Size([64])\n",
      "positional_encoding.encoder.5.running_var \t torch.Size([64])\n",
      "positional_encoding.encoder.5.num_batches_tracked \t torch.Size([])\n",
      "positional_encoding.encoder.6.weight \t torch.Size([128, 64, 1])\n",
      "positional_encoding.encoder.6.bias \t torch.Size([128])\n",
      "positional_encoding.encoder.8.weight \t torch.Size([128])\n",
      "positional_encoding.encoder.8.bias \t torch.Size([128])\n",
      "positional_encoding.encoder.8.running_mean \t torch.Size([128])\n",
      "positional_encoding.encoder.8.running_var \t torch.Size([128])\n",
      "positional_encoding.encoder.8.num_batches_tracked \t torch.Size([])\n",
      "positional_encoding.encoder.9.weight \t torch.Size([256, 128, 1])\n",
      "positional_encoding.encoder.9.bias \t torch.Size([256])\n",
      "attention_gnn.layers.0.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.0.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.0.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.0.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.0.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.0.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.0.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.0.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.0.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.0.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.0.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.0.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.0.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.0.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.0.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.0.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.0.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.1.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.1.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.1.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.1.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.1.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.1.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.1.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.1.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.1.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.1.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.1.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.1.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.1.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.1.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.1.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.1.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.1.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.2.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.2.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.2.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.2.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.2.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.2.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.2.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.2.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.2.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.2.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.2.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.2.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.2.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.2.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.2.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.2.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.2.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.3.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.3.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.3.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.3.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.3.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.3.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.3.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.3.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.3.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.3.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.3.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.3.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.3.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.3.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.3.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.3.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.3.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.4.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.4.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.4.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.4.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.4.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.4.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.4.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.4.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.4.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.4.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.4.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.4.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.4.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.4.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.4.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.4.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.4.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.5.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.5.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.5.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.5.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.5.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.5.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.5.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.5.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.5.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.5.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.5.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.5.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.5.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.5.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.5.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.5.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.5.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.6.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.6.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.6.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.6.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.6.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.6.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.6.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.6.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.6.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.6.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.6.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.6.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.6.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.6.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.6.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.6.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.6.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.7.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.7.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.7.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.7.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.7.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.7.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.7.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.7.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.7.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.7.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.7.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.7.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.7.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.7.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.7.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.7.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.7.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.8.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.8.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.8.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.8.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.8.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.8.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.8.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.8.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.8.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.8.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.8.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.8.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.8.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.8.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.8.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.8.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.8.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.9.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.9.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.9.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.9.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.9.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.9.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.9.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.9.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.9.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.9.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.9.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.9.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.9.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.9.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.9.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.9.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.9.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.10.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.10.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.10.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.10.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.10.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.10.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.10.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.10.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.10.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.10.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.10.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.10.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.10.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.10.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.10.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.10.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.10.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.11.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.11.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.11.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.11.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.11.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.11.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.11.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.11.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.11.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.11.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.11.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.11.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.11.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.11.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.11.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.11.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.11.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.12.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.12.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.12.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.12.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.12.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.12.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.12.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.12.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.12.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.12.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.12.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.12.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.12.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.12.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.12.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.12.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.12.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.13.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.13.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.13.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.13.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.13.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.13.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.13.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.13.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.13.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.13.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.13.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.13.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.13.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.13.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.13.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.13.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.13.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.14.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.14.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.14.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.14.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.14.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.14.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.14.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.14.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.14.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.14.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.14.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.14.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.14.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.14.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.14.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.14.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.14.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.15.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.15.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.15.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.15.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.15.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.15.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.15.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.15.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.15.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.15.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.15.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.15.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.15.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.15.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.15.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.15.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.15.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.16.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.16.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.16.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.16.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.16.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.16.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.16.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.16.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.16.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.16.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.16.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.16.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.16.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.16.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.16.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.16.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.16.module.fc.3.bias \t torch.Size([256])\n",
      "attention_gnn.layers.17.module.mha.in_proj_q.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.17.module.mha.in_proj_q.bias \t torch.Size([256])\n",
      "attention_gnn.layers.17.module.mha.in_proj_k.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.17.module.mha.in_proj_k.bias \t torch.Size([256])\n",
      "attention_gnn.layers.17.module.mha.in_proj_v.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.17.module.mha.in_proj_v.bias \t torch.Size([256])\n",
      "attention_gnn.layers.17.module.mha.out_proj.weight \t torch.Size([256, 256, 1])\n",
      "attention_gnn.layers.17.module.mha.out_proj.bias \t torch.Size([256])\n",
      "attention_gnn.layers.17.module.fc.0.weight \t torch.Size([512, 512, 1])\n",
      "attention_gnn.layers.17.module.fc.0.bias \t torch.Size([512])\n",
      "attention_gnn.layers.17.module.fc.2.weight \t torch.Size([512])\n",
      "attention_gnn.layers.17.module.fc.2.bias \t torch.Size([512])\n",
      "attention_gnn.layers.17.module.fc.2.running_mean \t torch.Size([512])\n",
      "attention_gnn.layers.17.module.fc.2.running_var \t torch.Size([512])\n",
      "attention_gnn.layers.17.module.fc.2.num_batches_tracked \t torch.Size([])\n",
      "attention_gnn.layers.17.module.fc.3.weight \t torch.Size([256, 512, 1])\n",
      "attention_gnn.layers.17.module.fc.3.bias \t torch.Size([256])\n",
      "linear_proj.weight \t torch.Size([256, 256, 1])\n",
      "linear_proj.bias \t torch.Size([256])\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img0, img1, lafs0, lafs1, inliers \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg0_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m draw_LAF_matches(\n\u001b[1;32m      3\u001b[0m     lafs0,\n\u001b[1;32m      4\u001b[0m     lafs1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtentative_color\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_color\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(output_dir)\n",
      "File \u001b[0;32m/workspace/code/OpenGlue/inference.py:235\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(image0_path, image1_path, experiment_path, checkpoint_name, device)\u001b[0m\n\u001b[1;32m    233\u001b[0m sg \u001b[38;5;241m=\u001b[39m OpenGlueMatcher(feature_extractor, matcher, config)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 235\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimg0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimg1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------\u001b[39m\u001b[38;5;124m'\u001b[39m, out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), cv2\u001b[38;5;241m.\u001b[39mUSAC_MAGSAC, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.999\u001b[39m, \u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m    237\u001b[0m F, inliers \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindFundamentalMat(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    238\u001b[0m                                     out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    239\u001b[0m                                     cv2\u001b[38;5;241m.\u001b[39mUSAC_MAGSAC, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.999\u001b[39m, \u001b[38;5;241m100000\u001b[39m)\n",
      "File \u001b[0;32m~/code/OpenGlue/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/code/OpenGlue/inference.py:150\u001b[0m, in \u001b[0;36mOpenGlueMatcher.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    data: dictionary containing the input data in the following format:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    - ``batch_indexes``, batch indexes for the keypoints and lafs :math:`(NC)`.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlafs0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# One can supply pre-extracted local features\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     feats_dict0: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     lafs0, descs0, resps0 \u001b[38;5;241m=\u001b[39m feats_dict0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlafs\u001b[39m\u001b[38;5;124m'\u001b[39m], feats_dict0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors\u001b[39m\u001b[38;5;124m'\u001b[39m], feats_dict0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/code/OpenGlue/inference.py:118\u001b[0m, in \u001b[0;36mOpenGlueMatcher.extract_features\u001b[0;34m(self, image, mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    115\u001b[0m                      image: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    116\u001b[0m                      mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124;03m\"\"\"Function for feature extraction from simple image.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     lafs0, resps0, descs0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlafs\u001b[39m\u001b[38;5;124m\"\u001b[39m: lafs0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m: resps0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptors\u001b[39m\u001b[38;5;124m\"\u001b[39m: descs0}\n",
      "File \u001b[0;32m~/code/OpenGlue/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/code/OpenGlue/models/features/superpoint/model.py:84\u001b[0m, in \u001b[0;36mSuperPointNet.forward\u001b[0;34m(self, image, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"Compute keypoints, scores, descriptors for image\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m descriptors, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m b, _, h, w \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     87\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(b, h, w, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m/workspace/code/OpenGlue/models/features/superpoint/model.py:65\u001b[0m, in \u001b[0;36mSuperPointNet._forward_layers\u001b[0;34m(self, image, mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))(x))\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m~/code/OpenGlue/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/code/OpenGlue/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/OpenGlue/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "img0, img1, lafs0, lafs1, inliers = inference.run_inference(img0_path, img1_path, experiment_path, checkpoint_name, device)\n",
    "draw_LAF_matches(\n",
    "    lafs0,\n",
    "    lafs1,\n",
    "    torch.arange(len(inliers)).view(-1, 1).repeat(1, 2),\n",
    "    K.tensor_to_image(img0),\n",
    "    K.tensor_to_image(img1),\n",
    "    inliers,\n",
    "    draw_dict={'inlier_color': (0.2, 1, 0.2),\n",
    "               'tentative_color': None,\n",
    "               'feature_color': (0.2, 0.5, 1), 'vertical': True})\n",
    "plt.savefig(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d41c5a-9e70-4bdf-b799-3b38042dcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f79911c9-7a3b-4b36-8d66-f75d5288154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8763c08-b212-4f32-a30a-3b90b6861639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118de8f-9630-4435-b75a-3861f2f312cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(venv) OpenGlue",
   "language": "python",
   "name": "openglue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
