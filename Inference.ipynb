{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53252ea-b226-4b58-ae28-fd62394d5379",
   "metadata": {},
   "source": [
    "## Inference\n",
    "To test trained models, run inference between 2 images using the model and analyze the results. Use the experiment path outputted in the training step to reference the location where the configurations and checkpoints that are to be used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51fc2ae-13de-41a0-b715-1b6cde9b9be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inference\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#Where the images to run inference on are\n",
    "img0_path = 'imgs/test_images/img2.jpg'\n",
    "img1_path = 'imgs/test_images/img3.jpg'\n",
    "\n",
    "#Where the configurations and checkpoints saved during training are\n",
    "\n",
    "#trained with DataParallelStrategy in Jupyter\n",
    "#trained with cached features\n",
    "#experiment_path = '/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3/SuperPointNet_960_720_preextracted/SuperPointNet_cache__attn_softmax__laf_none__2022-06-15-21-32-49'\n",
    "\n",
    "#trained without cached features\n",
    "experiment_path = '/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3/Superpoint_960_720/SuperPointNet__attn_softmax__laf_none__2022-06-15-22-13-22'\n",
    "\n",
    "#trained with DDPStrategy in non-interactive openglue environment, inference works after correcting incorrectly loaded local_features_extractor weights and biases from statedict\n",
    "#experiment_path = '/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3/Superpoint_960_720/SuperPointNet__attn_softmax__laf_none__2022-06-15-22-13-22'\n",
    "\n",
    "#name of the checkpoint file stored in the experiment path directory\n",
    "checkpoint_name = 'superglue-step=750.ckpt'\n",
    "\n",
    "#which device to use to run inference (cuda or cpu)\n",
    "device = 'cuda'\n",
    "\n",
    "#output location\n",
    "output_dir = 'results.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d88a01-0508-4605-a6a2-22157e034778",
   "metadata": {},
   "source": [
    "- Config.yaml in 'experiment_path' directory\n",
    "    - Under superglue:\n",
    "        - If unset, add 'descriptor_dim: <dim>' where <dim> is the same as the corresponding field in features_config.yaml in the same folder (usually 256)\n",
    "        - Under positional_encoding:\n",
    "            - If unset, add and set 'output_size: ' to the same value\n",
    "        - Under attention_gnn:\n",
    "            - If unset, add and set 'embed_dim: ' to the same value\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3d76bf-7710-430d-bd69-c6eadc2c3143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venvs/test_OpenGlue/lib/python3.8/site-packages/torch/cuda/__init__.py:145: UserWarning: \n",
      "NVIDIA GeForce RTX 3080 Ti Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3080 Ti Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''From inference.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m    Modified to use direct variable input rather than command line arguments\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m img0, img1, lafs0, lafs1, inliers \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg0_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg1_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m draw_LAF_matches(\n\u001b[1;32m      8\u001b[0m     lafs0,\n\u001b[1;32m      9\u001b[0m     lafs1,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtentative_color\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_color\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(output_dir)\n",
      "File \u001b[0;32m/workspace/code/test/OpenGlue/inference.py:242\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(image0_path, image1_path, experiment_path, checkpoint_name, device)\u001b[0m\n\u001b[1;32m    240\u001b[0m sg \u001b[38;5;241m=\u001b[39m OpenGlueMatcher(feature_extractor, matcher, config)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 242\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimg0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimg1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m F, inliers \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mfindFundamentalMat(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    244\u001b[0m                                     out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoints1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    245\u001b[0m                                     cv2\u001b[38;5;241m.\u001b[39mUSAC_MAGSAC, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.999\u001b[39m, \u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m    246\u001b[0m inliers \u001b[38;5;241m=\u001b[39m inliers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/workspace/.venvs/test_OpenGlue/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/code/test/OpenGlue/inference.py:157\u001b[0m, in \u001b[0;36mOpenGlueMatcher.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    data: dictionary containing the input data in the following format:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    - ``batch_indexes``, batch indexes for the keypoints and lafs :math:`(NC)`.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlafs0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# One can supply pre-extracted local features\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     feats_dict0: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     lafs0, descs0, resps0 \u001b[38;5;241m=\u001b[39m feats_dict0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlafs\u001b[39m\u001b[38;5;124m'\u001b[39m], feats_dict0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescriptors\u001b[39m\u001b[38;5;124m'\u001b[39m], feats_dict0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/code/test/OpenGlue/inference.py:125\u001b[0m, in \u001b[0;36mOpenGlueMatcher.extract_features\u001b[0;34m(self, image, mask)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    122\u001b[0m                      image: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    123\u001b[0m                      mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124;03m\"\"\"Function for feature extraction from simple image.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     lafs0, resps0, descs0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlafs\u001b[39m\u001b[38;5;124m\"\u001b[39m: lafs0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponses\u001b[39m\u001b[38;5;124m\"\u001b[39m: resps0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescriptors\u001b[39m\u001b[38;5;124m\"\u001b[39m: descs0}\n",
      "File \u001b[0;32m/workspace/.venvs/test_OpenGlue/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/code/test/OpenGlue/models/features/superpoint/model.py:84\u001b[0m, in \u001b[0;36mSuperPointNet.forward\u001b[0;34m(self, image, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"Compute keypoints, scores, descriptors for image\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m descriptors, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m b, _, h, w \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     87\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(b, h, w, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m/workspace/code/test/OpenGlue/models/features/superpoint/model.py:65\u001b[0m, in \u001b[0;36mSuperPointNet._forward_layers\u001b[0;34m(self, image, mask)\u001b[0m\n\u001b[1;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m image\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))(x))\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[0;32m/workspace/.venvs/test_OpenGlue/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/.venvs/test_OpenGlue/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/.venvs/test_OpenGlue/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "'''From inference.py\n",
    "    Modified to use direct variable input rather than command line arguments\n",
    "'''\n",
    "\n",
    "\n",
    "img0, img1, lafs0, lafs1, inliers = inference.run_inference(img0_path, img1_path, experiment_path, checkpoint_name, device)\n",
    "draw_LAF_matches(\n",
    "    lafs0,\n",
    "    lafs1,\n",
    "    torch.arange(len(inliers)).view(-1, 1).repeat(1, 2),\n",
    "    K.tensor_to_image(img0),\n",
    "    K.tensor_to_image(img1),\n",
    "    inliers,\n",
    "    draw_dict={'inlier_color': (0.2, 1, 0.2),\n",
    "               'tentative_color': None,\n",
    "               'feature_color': (0.2, 0.5, 1), 'vertical': True})\n",
    "plt.savefig(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cba76d-fe70-43aa-af80-3c433e13db4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_OpenGlue",
   "language": "python",
   "name": "test_openglue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
