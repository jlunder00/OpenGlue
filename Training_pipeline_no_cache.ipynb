{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f564a68-49d4-4270-8fa2-e6f0ab958f21",
   "metadata": {},
   "source": [
    "# OpenGlue Pipeline with no pre-extraction\n",
    "In this pipeline, we train the model without preforming feature extraction prior to training. Instead, feature extraction is done during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd9428-c9d1-4618-aff7-6adc3fac088e",
   "metadata": {},
   "source": [
    "# Training with no pre-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc9a36-b21c-4187-b7c6-3682ce20e6f0",
   "metadata": {},
   "source": [
    "#### Configuration Options\n",
    "\n",
    "See [`CONFIGURATIONS.md`](CONFIGURATIONS.md) for configuration details. Please ensure all config options are set properly prior to training. For no pre-extraction, config/config.yaml will be used as default, but if you specify a different config file in the arguments, then it will be merged with config/config.yaml, overwritting such that the settings in the specified config file are kept over the conflicting ones in config/config.yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b844d-6f5f-4522-9380-3f24b7da4027",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Selecting a Feature Extractor\n",
    "\n",
    "To choose a specific feature extractor for pre-extraction or realtime extraction, you specify the filepath to the config file for that feature extractor, containing neccessary configurations and a filepath to the model weights if applicable. 3 of these extractors, the superpoint extractors, use pretrained models, while the others do not. It is reccommended to use one of these 3: superpoint_magicleap, superpoint_kitti, or superpoint_coco. These are differentiated by the dataset that was used to train them. 2 sets of default extractors are included, one in config/features/ and one in config/features_online/<br />\n",
    "In config/features/, the extractors are configured to run with a higher number of maximum keypoints. For the superpoint extractors, this is set at 2048 keypoints. In features online, the extractors are configured to run with a lower number of maximum keypoints, which helps reduce runtime and memory utilization. For the superpoint extractors, this is 1024 keypoints. <br/><br/>\n",
    "For training without cached features, maximum keypoints are capped at 1024, so it is reccommended to choose a feature extractor from config/features_online/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b4874d-de3c-4e72-877c-5495eba0be10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code/OpenGlue/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#From train.py\n",
    "'''\n",
    "    Modified strategy to make compatible with interactive python runtime\n",
    "    Modified input from command line via argparse to directly passed variables more suitable for jupyter\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import shutup\n",
    "\n",
    "shutup.please()\n",
    "import os\n",
    "from datetime import datetime\n",
    "from omegaconf import OmegaConf\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#Modified from importing DDPPlugin (depreciates)\n",
    "from pytorch_lightning.strategies import DataParallelStrategy\n",
    "\n",
    "from data.megadepth_datamodule import MegaDepthPairsDataModule\n",
    "from models.matching_module import MatchingTrainingModule\n",
    "from utils.train_utils import get_training_loggers, get_training_callbacks, prepare_logging_directory\n",
    "\n",
    "\n",
    "def train(config_path, features_config_path):\n",
    "    # Load config\n",
    "    config = OmegaConf.load('config/config.yaml')  # base config\n",
    "    feature_extractor_config = OmegaConf.load(features_config_path)\n",
    "    if config_path != 'config/config.yaml':\n",
    "        add_conf = OmegaConf.load(config_path)\n",
    "        config = OmegaConf.merge(config, add_conf)\n",
    "\n",
    "    pl.seed_everything(int(os.environ.get('LOCAL_RANK', 0)))\n",
    "\n",
    "    # Prepare directory for logs and checkpoints\n",
    "    # Added pathing option in config file to give ability to move experiment location out of log directory through 'experiments_root_path'\n",
    "    if os.environ.get('LOCAL_RANK', 0) == 0:\n",
    "        experiment_name = '{}__attn_{}__laf_{}__{}'.format(\n",
    "            #config['data']['experiments_root_path'],\n",
    "            feature_extractor_config['name'],\n",
    "            config['superglue']['attention_gnn']['attention'],\n",
    "            config['superglue']['laf_to_sideinfo_method'],\n",
    "            str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        )\n",
    "        log_path = prepare_logging_directory(config, experiment_name, features_config=feature_extractor_config)\n",
    "    else:\n",
    "        experiment_name, log_path = '', ''\n",
    "\n",
    "    # Init Lightning Data Module\n",
    "    data_config = config['data']\n",
    "    dm = MegaDepthPairsDataModule(\n",
    "        root_path=data_config['root_path'],\n",
    "        train_list_path=data_config['train_list_path'],\n",
    "        val_list_path=data_config['val_list_path'],\n",
    "        test_list_path=data_config['test_list_path'],\n",
    "        batch_size=data_config['batch_size_per_gpu'],\n",
    "        num_workers=data_config['dataloader_workers_per_gpu'],\n",
    "        target_size=data_config['target_size'],\n",
    "        val_max_pairs_per_scene=data_config['val_max_pairs_per_scene'],\n",
    "        train_pairs_overlap=data_config.get('train_pairs_overlap')\n",
    "    )\n",
    "\n",
    "    # Init model\n",
    "    model = MatchingTrainingModule(\n",
    "        train_config={**config['train'], **config['inference'], **config['evaluation']},\n",
    "        features_config=feature_extractor_config,\n",
    "        superglue_config=config['superglue'],\n",
    "    )\n",
    "\n",
    "    # Set callbacks and loggers\n",
    "    callbacks = get_training_callbacks(config, log_path, experiment_name)\n",
    "    loggers = get_training_loggers(config, log_path, experiment_name)\n",
    "\n",
    "    # Init trainer\n",
    "    # Replace accelerator='ddp' with gpu, and replace plugins=DDPPlugin() with strategy=DataParallelStrategy() to comply with\n",
    "    # Pytorch lightning updates and make model compatible with interactive python runtime\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=config['gpus'],\n",
    "        max_epochs=config['train']['epochs'],\n",
    "        \n",
    "        #Vital, with any other accelerator than gpu or no accelerator (while using DataParallelStrategy), \n",
    "        #resultant model will not be usable for inference test\n",
    "        accelerator=\"gpu\", \n",
    "        \n",
    "        gradient_clip_val=config['train']['grad_clip'],\n",
    "        log_every_n_steps=config['logging']['train_logs_steps'],\n",
    "        limit_train_batches=config['train']['steps_per_epoch'],\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=callbacks,\n",
    "        logger=loggers,\n",
    "        strategy=DataParallelStrategy(),\n",
    "        #plugins=DDPPlugin(find_unused_parameters=False),\n",
    "        precision=config['train'].get('precision', 32),\n",
    "    )\n",
    "    # If loaded from checkpoint - validate\n",
    "    if config.get('checkpoint') is not None:\n",
    "        trainer.validate(model, datamodule=dm, ckpt_path=config.get('checkpoint'))\n",
    "    trainer.fit(model, datamodule=dm, ckpt_path=config.get('checkpoint'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5855e-1ea7-4ae2-858b-c5fcdd607ccd",
   "metadata": {},
   "source": [
    "##### Ensure torch in interactive python is working properly\n",
    "Sometimes, torch running inside of an interactive python environment fails to recognize cuda devices. If you have a cuda device you expect to be detected, run this to make sure that torch finds it. If this does not display the expected number of devices, especially if it detects none, try restarting the container that the interactive environment is running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60642dca-8195-4740-a090-abff55094bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices found:  1\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices found: ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366791e-f2cd-4f1f-9ad4-a6b6d60cb83f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Set the path to the config files you would like to use for training with realtime extraction and begin training in the interactive environment below. <br />Alternatively,\n",
    "<b>To launch train as a script with local feature extraction throughout training, run: </b>  \n",
    "```\n",
    "python train.py --config='config/config.yaml' --features_config='config/features_online/sift.yaml'\n",
    "```\n",
    "Note: sift is an example, you may use any of the config files present or one you create yourself. The Superpoint configs are reccommended.\n",
    "This will utilize DDPStrategy (Distributed Data Parallel), as opposed to DataParallelStrategy. In the interactive environment, DDPStrategy is incompatible, so DataParallelStrategy is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95677dc-b244-4230-82d5-25f5563ad260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m features_config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./config/features/superpoint_magicleap.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./config/config.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m(config_path, features_config_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "features_config_path = './config/features/superpoint_magicleap.yaml'\n",
    "config_path = './config/config.yaml'\n",
    "train(config_path, features_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da3df336-9036-442f-a68e-df1e680eb923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f249e7f5-0722-467e-86f4-bde11329a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(venv) OpenGlue",
   "language": "python",
   "name": "openglue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
