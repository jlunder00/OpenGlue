{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fd14e6b-3e3f-44d7-b66f-9e84ac060a4f",
   "metadata": {},
   "source": [
    "# Training with Cached Features\n",
    "With this option, we preform feature extraction as a separate step prior to training and save the features so they can be used in training. This improves training runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189db336-5982-4fb6-9fae-f6138a93affd",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "You can run feature extraction as a separate step, prior to training. This will make training run faster, but is very expensive in terms of disk space and time, as there are not any built in options inside of the OpenGlue framework to preform feature extraction on a subset of the data. The only ways to do this are to either manually move all of the scene folders not included in the files specifying the subsets generated in the previous step to a different location, or to not run feature extraction as a separate step and instead run OpenGlue training such that feature extraction is done concurrently with training, meaning only the scene files used for training/validation will have feature extraction preformed on them.\n",
    "<br />The steps to do feature extraction as a separate step are detailed in the README:<br />\n",
    "run `python extract_features.py` with the following parameters (for more details, please, refer to module's documentation):\n",
    "      \n",
    "   * `--device` - `[cpu, cuda]`\n",
    "   * `--num_workers` - number of workers for parallel processing. When cpu is chosen, assigns the exact amount of the set workers, for gpu scenario it takes into account the number of gpu units available. \n",
    "   * `--target_size` - target size of the image (WIDTH, HEIGHT).\n",
    "   * `--data_path` - path to directory with scenes images\n",
    "   * `--output_path` - path to directory where extracted features are stored\n",
    "   * `--extractor_config_path` - 'path to the file containing config for feature extractor in .yaml format\n",
    "   * `--recompute` - include this flag to recompute features if it is already present in output directory\n",
    "   * `--image_format` - formats of images searched inside `data_path` to compute features for, default: [jpg, JPEG, JPG, png]\n",
    "   \n",
    "   Choosing local feature extractor is performed via `--extractor_config_path`. \n",
    "   We provide configs in `config/features/`, which user can edit or use unchanged:\n",
    "   * SuperPoint with MagicLeap weights - [`superpoint_magicleap.yaml`](config/features/superpoint_magicleap.yaml)\n",
    "   * SuperPoint with KITTI weights - [`superpoint_kitti.yaml`](config/features/superpoint_kitti.yaml)\n",
    "   * SuperPoint with COCO weights - [`superpoint_coco.yaml`](config/features/superpoint_coco.yaml)\n",
    "   * SIFT opencv - [`sift_opencv.yaml`](config/features/sift_opencv.yaml)\n",
    "   * DoG-AffNet-HardNet - [`dog_opencv_affnet_hardnet.yaml`](config/features/dog_opencv_affnet_hardnet.yaml) Note: I would reccommend this not be used\n",
    "<br />I have included an example command to run this from the command line, as well as reimplemented the necessary functions such that it can be run from here, with the arguments being in a list rather than entered as command line arguments. The command line version would look like:<br /> `python extract_features.py --device 'cuda' --num_workers '1' --target_size 1280 1280 --data_path '/host_Data/Data/MegaDepth/MegaDepth/phoenix/S6/zl548/MegaDepth_v1/' --output_path '/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix' --extractor_config_path 'config/features/superpoint_magicleap.yaml'`<br />\n",
    "Note: if feature extraction is run as a separate step, be sure to take note of the target size you use. In this case it is 1280 by 1280, which is what I used when I ran feature extraction on it's own, but this value needs to be consistent inside the config files to be used during training. Also, this value could be made smaller, which may improve runtimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7c2f9-b836-4f01-b939-8d4a80cad9f3",
   "metadata": {},
   "source": [
    "#### Selecting a Feature Extractor\n",
    "\n",
    "To choose a specific feature extractor for pre-extraction or realtime extraction, you specify the filepath to the config file for that feature extractor, containing neccessary configurations and a filepath to the model weights if applicable. 3 of these extractors, the superpoint extractors, use pretrained models, while the others do not. It is reccommended to use one of these 3: superpoint_magicleap, superpoint_kitti, or superpoint_coco. These are differentiated by the dataset that was used to train them. 2 sets of default extractors are included, one in config/features/ and one in config/features_online/<br />\n",
    "In config/features/, the extractors are configured to run with a higher number of maximum keypoints. For the superpoint extractors, this is set at 2048 keypoints. In features online, the extractors are configured to run with a lower number of maximum keypoints, which helps reduce runtime and memory utilization. For the superpoint extractors, this is 1024 keypoints. <br/><br/>\n",
    "For training with cached-features, you must also specify the maximum number of keypoints in the configuration file used for training. This should be less than or equal to the number used in feature extraction, though it is reccommended that they be the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0eb33e9-f643-438c-b30b-4789384bf073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code/OpenGlue/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#From: extract_features.py\n",
    "'''\n",
    "Modified to not use argparse in interactive environment and to allow for only preforming feature extraction on a subset of the data\n",
    "'''\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import math\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Union, Optional\n",
    "\n",
    "import cv2\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "from torch import multiprocessing\n",
    "\n",
    "from models.features import get_feature_extractor\n",
    "\n",
    "from multiprocessing_helper import process_chunk\n",
    "import extract_features\n",
    "\n",
    "def get_images_list(input_data_path: pathlib.Path, image_formats: List[str], scene_subset_path: pathlib.Path) -> List[Tuple[str, Optional[str]]]:\n",
    "    \"\"\"\n",
    "    Get list of images that wil be processed.\n",
    "    Args:\n",
    "        input_data_path: input path to the location with images\n",
    "        image_formats: file formats of images to look for\n",
    "        scence_subset_path: input path to file specifying which scenes to use. All scenes used if set to ./None\n",
    "    Returns:\n",
    "        images_path_list: list where each image is represented as tuple with path to image\n",
    "        and scene name (or None if no scenes are available)\n",
    "    \"\"\"\n",
    "    \n",
    "    scene_set = []\n",
    "    if not scene_subset_path.match(\"./None\"):\n",
    "        with open(scene_subset_path, 'r') as fin:\n",
    "            scene_set = fin.read().split('\\n')\n",
    "    print(scene_set)\n",
    "    # process each scene\n",
    "    images_path_list = []\n",
    "    \n",
    "    #Added to take subset prior to extraction\n",
    "    scenes = os.listdir(input_data_path)\n",
    "    for scene in scenes:\n",
    "        if scene not in scene_set and len(scene_set) > 0:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        images_path = input_data_path / scene / 'dense0' / 'imgs'\n",
    "        scene_list = []\n",
    "        for image_format in image_formats:\n",
    "            scene_list.extend(glob.glob(str(images_path / f'*.{image_format}')))\n",
    "        images_path_list.extend(((path, scene) for path in scene_list))\n",
    "    return images_path_list\n",
    "\n",
    "def get_output_directory_name(feature_extractor_config: dict, target_size) -> str:\n",
    "    \"\"\"\n",
    "    Build output directory name based on parameters\n",
    "    Args:\n",
    "        feature_extractor_config: parameters of feature extractor\n",
    "        args: command line arguments\n",
    "\n",
    "    Returns:\n",
    "        Name of directory where features are stored\n",
    "    \"\"\"\n",
    "    name = feature_extractor_config['name']\n",
    "    if target_size is not None:\n",
    "        name += f'_{target_size[0]}_{target_size[1]}'\n",
    "    return name\n",
    "\n",
    "\n",
    "def extract(data_path, output_path, device='cpu', num_workers=1, target_size=None, extractor_config_path=Path('config/features/sift_opencv.yaml'), image_format=['jpg', 'JPEG', 'JPG', 'png'], scene_subset_path=Path('./None'), recompute=False):\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if device == 'cuda' and torch.cuda.device_count() < num_workers:\n",
    "        logger.warning(f'Number of workers selected is bigger than number of available cuda devices. '\n",
    "                       f'Setting num_workers to {torch.cuda.device_count()}.')\n",
    "        num_workers = torch.cuda.device_count()\n",
    "\n",
    "    # read feature extractor config\n",
    "    with open(extractor_config_path) as f:\n",
    "        feature_extractor_config = yaml.full_load(f)\n",
    "\n",
    "    # make output directory\n",
    "    output_path = output_path / get_output_directory_name(feature_extractor_config, target_size)\n",
    "    logger.info(f'Creating output directory {output_path} (if not exists).')\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    with open(os.path.join(output_path, 'config.yaml'), 'w') as f:\n",
    "        yaml.dump(feature_extractor_config, f)\n",
    "\n",
    "\n",
    "    #modified to add scene_subset_path to preform extraction on only a subset of data specified in the file at scene_subset_path\n",
    "    images_list = get_images_list(data_path, image_format, scene_subset_path)\n",
    "    logger.info(f'Total number of images found to process: {len(images_list)}')\n",
    "    # split into chunks of (almost) equal size\n",
    "    chunk_size = math.ceil(len(images_list) / num_workers)\n",
    "    images_list = [images_list[i * chunk_size:(i + 1) * chunk_size] for i in range(num_workers)]\n",
    "\n",
    "    logger.info(f'Starting {num_workers} processes for features extraction.')\n",
    "    multiprocessing.start_processes(\n",
    "        process_chunk,\n",
    "        args=(images_list, feature_extractor_config, output_path, device, recompute, target_size),\n",
    "        nprocs=num_workers,\n",
    "        join=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d71017-38c3-4260-8300-71aa56eebc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022/06/15 07:16:32] __main__ | INFO: Creating output directory /host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3/SuperPointNet_960_720 (if not exists).\n",
      "[2022/06/15 07:16:32] __main__ | INFO: Total number of images found to process: 2147\n",
      "[2022/06/15 07:16:32] __main__ | INFO: Starting 1 processes for features extraction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0024', '0048', '0102', '0148', '0181', '0307', '']\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "num_workers = 1\n",
    "target_size = [960, 720]\n",
    "data_path = Path('/host_Data/Data/MegaDepth/MegaDepth/phoenix/S6/zl548/MegaDepth_v1/')\n",
    "output_path = Path('/host_Data/Data/MegaDepth/MegaDepth/extracted_features/phoenix3')\n",
    "extractor_config_path = Path('./config/features_online/superpoint_magicleap.yaml')\n",
    "scene_subset_path = Path('./assets/subset-20-20-total.txt')\n",
    "#scene_subset_path = Path('./None') #Use to preform extraction on the entire dataset\n",
    "extract(device=device, num_workers=num_workers, target_size=target_size, data_path=data_path, output_path=output_path, extractor_config_path=extractor_config_path, scene_subset_path=scene_subset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5466de7-720b-4587-a7de-520bfefa1bfb",
   "metadata": {},
   "source": [
    "#### Configuration Options\n",
    "\n",
    "See [`CONFIGURATIONS.md`](CONFIGURATIONS.md) for configuration details. Please ensure all config options are set properly prior to training. For pre-extraction (cached features), config/config_cached.yaml will be used as default, but if you specify a different config file in the arguments, then it will be merged with config/config_cached.yaml, overwritting such that the settings in the specified config file are kept over the conflicting ones in config/config_cached.yaml. So, modify config_cached.yaml or create your own to use with the proper config options set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0bcea3-7249-49f4-ba65-1614d9230c88",
   "metadata": {},
   "source": [
    "## Training with pre-extraction (cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d807eb-ab21-499e-bcac-bcefcb263812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code/OpenGlue/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#from train_cached.py\n",
    "\n",
    "import torch\n",
    "import shutup\n",
    "\n",
    "shutup.please()\n",
    "import os\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from omegaconf import OmegaConf\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DataParallelStrategy\n",
    "\n",
    "from data.megadepth_datamodule import MegaDepthPairsDataModuleFeatures\n",
    "from models.matching_module import MatchingTrainingModule\n",
    "from utils.train_utils import get_training_loggers, get_training_callbacks, prepare_logging_directory\n",
    "\n",
    "\n",
    "def train_cached(config_path='config/config_cached.yaml'):\n",
    "\n",
    "    # Load config\n",
    "    config = OmegaConf.load('config/config_cached.yaml')  # base config\n",
    "    if config_path != 'config/config_cached.yaml':\n",
    "        add_conf = OmegaConf.load(config_path)\n",
    "        config = OmegaConf.merge(config, add_conf)\n",
    "\n",
    "    pl.seed_everything(int(os.environ.get('LOCAL_RANK', 0)))\n",
    "    \n",
    "    # moved assignment of features_config before experiment_name creation to facilitate correcting error inputting\n",
    "    # the entire path to the extracted features being used as part of the experiment name\n",
    "    features_config = OmegaConf.load(os.path.join(config['data']['root_path'],\n",
    "                                               config['data']['features_dir'], 'config.yaml'))\n",
    "\n",
    "    # Prepare directory for logs and checkpoints\n",
    "    # Use features_config['name'] rather than config['data']['features_dir'] to prevent entire path\n",
    "    # from being inputted as part of the experiment name, resulting in logs being saved in an unexpected location\n",
    "    if os.environ.get('LOCAL_RANK', 0) == 0:\n",
    "        experiment_name = '{}_cache__attn_{}__laf_{}__{}'.format(\n",
    "            features_config['name'],\n",
    "            config['superglue']['attention_gnn']['attention'],\n",
    "            config['superglue']['laf_to_sideinfo_method'],\n",
    "            str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        )\n",
    "        log_path = prepare_logging_directory(config, experiment_name)\n",
    "    else:\n",
    "        experiment_name, log_path = '', ''\n",
    "    print(experiment_name, log_path)\n",
    "\n",
    "    # Init Lightning Data Module\n",
    "    data_config = config['data']\n",
    "    dm = MegaDepthPairsDataModuleFeatures(\n",
    "        root_path=data_config['root_path'],\n",
    "        train_list_path=data_config['train_list_path'],\n",
    "        val_list_path=data_config['val_list_path'],\n",
    "        test_list_path=data_config['test_list_path'],\n",
    "        batch_size=data_config['batch_size_per_gpu'],\n",
    "        num_workers=data_config['dataloader_workers_per_gpu'],\n",
    "        target_size=data_config['target_size'],\n",
    "        features_dir=data_config['features_dir'],\n",
    "        num_keypoints=data_config['max_keypoints'],\n",
    "        val_max_pairs_per_scene=data_config['val_max_pairs_per_scene'],\n",
    "        balanced_train=data_config.get('balanced_train', False),\n",
    "        train_pairs_overlap=data_config.get('train_pairs_overlap')\n",
    "    )\n",
    "\n",
    "\n",
    "    # Init model\n",
    "    model = MatchingTrainingModule(\n",
    "        train_config={**config['train'], **config['inference'], **config['evaluation']},\n",
    "        features_config=features_config,\n",
    "        superglue_config=config['superglue'],\n",
    "    )\n",
    "\n",
    "    # Set callbacks and loggers\n",
    "    callbacks = get_training_callbacks(config, log_path, experiment_name)\n",
    "    loggers = get_training_loggers(config, log_path, experiment_name)\n",
    "\n",
    "    # Init trainer\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=config['gpus'],\n",
    "        max_epochs=config['train']['epochs'],\n",
    "        accelerator=\"gpu\",\n",
    "        gradient_clip_val=config['train']['grad_clip'],\n",
    "        log_every_n_steps=config['logging']['train_logs_steps'],\n",
    "        limit_train_batches=config['train']['steps_per_epoch'],\n",
    "        num_sanity_val_steps=5,\n",
    "        callbacks=callbacks,\n",
    "        logger=loggers,\n",
    "        strategy=DataParallelStrategy(),\n",
    "        #plugins=DDPPlugin(find_unused_parameters=False),\n",
    "        precision=config['train'].get('precision', 32),\n",
    "    )\n",
    "    # If loaded from checkpoint - validate\n",
    "    if config.get('checkpoint') is not None:\n",
    "        trainer.validate(model, datamodule=dm, ckpt_path=config.get('checkpoint'))\n",
    "    trainer.fit(model, datamodule=dm, ckpt_path=config.get('checkpoint'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57479872-b2e6-49a6-a9de-339844598bd1",
   "metadata": {},
   "source": [
    "##### Ensure torch in interactive python is working properly\n",
    "Sometimes, torch running inside of an interactive python environment fails to recognize cuda devices. If you have a cuda device you expect to be detected, run this to make sure that torch finds it. If this does not display the expected number of devices, especially if it detects none, try restarting the container that the interactive environment is running on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d50f3a-0470-4b78-bb40-d1ba730c79ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices found:  1\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices found: ', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6b9d4-9746-4141-8a85-ef0041d8fb04",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Set the path to the config file you would like to use for training with cached features and begin training in the interactive environment below. config/config_cached.yaml is used by default <br />Alternatively,\n",
    "<b>To launch train as a script with cached features, run: </b>  \n",
    "```\n",
    "python train_cached.py --config='config/config_cached.yaml'\n",
    "```\n",
    "This will utilize DDPStrategy (Distributed Data Parallel), as opposed to DataParallelStrategy. In the interactive environment, DDPStrategy is incompatible, so DataParallelStrategy is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d3ec5c-f8ab-4533-8608-5c19dae3fd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs SuperPointNet_960_720_preextracted SuperPointNet_cache__attn_softmax__laf_none__2022-06-15-17-11-06\n",
      "log path logs/SuperPointNet_960_720_preextracted/SuperPointNet_cache__attn_softmax__laf_none__2022-06-15-17-11-06\n",
      "SuperPointNet_cache__attn_softmax__laf_none__2022-06-15-17-11-06 logs/SuperPointNet_960_720_preextracted/SuperPointNet_cache__attn_softmax__laf_none__2022-06-15-17-11-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlunder\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/code/OpenGlue/wandb/run-20220615_171107-4osofu6e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jlunder/superglue/runs/4osofu6e\" target=\"_blank\">SuperPointNet_cache__attn_softmax__laf_none__2022-06-15-17-11-06</a></strong> to <a href=\"https://wandb.ai/jlunder/superglue\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                   | Type                      | Params\n",
      "---------------------------------------------------------------------\n",
      "0 | superglue              | SuperGlue                 | 12.0 M\n",
      "1 | augmentations          | AugmentationSequential    | 0     \n",
      "2 | epipolar_dist_metric   | AccuracyUsingEpipolarDist | 0     \n",
      "3 | camera_pose_auc_metric | CameraPoseAUC             | 0     \n",
      "---------------------------------------------------------------------\n",
      "12.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.0 M    Total params\n",
      "47.829    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "config = 'config/config_cached.yaml'\n",
    "train_cached(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(venv) OpenGlue",
   "language": "python",
   "name": "openglue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
